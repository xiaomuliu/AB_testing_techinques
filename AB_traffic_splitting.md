### 综述
在执行A/B实验前，我们经常会被问到这样问题：我们可以用10%对照组vs.90%实验组的流量分配，而不是通常的50-50分流比例吗？一般的最小样本量计算工具通常会假设不同组之间是相同的样本量。如果在一个实验中一开始用小流量比如1%:1%分流，然后在进行一段时间后，出于某种原因调整为10%:90%。当用不同比例分流时会发生什么呢？我们先考虑在实验一开始就设定不平衡的分流比例，同时在实验进行中不调整流量的情况，再讨论在实验进行中调整流量的问题。

### 分流比例
我们在施加一个干预（treatment）后，希望从样本中估计这个干预变量带来的总体的指标差异。我们希望通过A/B实验的方式来得到这个无偏估计。我们知道评估一个估计量（estimator）的一般从无偏性（unbiasedness）, 一致性（consistency），有效性（efficiency）, 有时候还会加上渐进正态性（asymptotic normality）等角度入手。

首先不平衡的样本量会带来有偏误（biased）的结论吗？答案是否定的。只要实验组和对照组满足分组相互独立这个A/B实验基本前提，指标在组间差异的估计量就仍然是无偏的且一致的。同时依赖抽样分布为渐进正态的参数方法进行的假设检验，渐进正态性也不受影响。注意：在实验进行中调整流量比例有可能引入偏误，具体问题在下个小节讨论。

其次不平衡的样本量的有效性如何？我们通过一个模拟的例子来解释：
假设我们测试某个比例型转化指标（比如UV点击率），实验组和对照组样本量均为10000, 指标值对照组为10%, 实验组为11.5%, 那么在$ \alpha=0.05 $双边检验显著性水平上，假设两组在总体上真实存在差异，我们有多大能力检验出来呢？就是统计功效（power）是多少呢？我们根据上述信息计算
```r
library(pwr)
n1 = 10000
n2 = 10000
p1 = 0.10
p2 = 0.115
h = abs(2*asin(sqrt(p1))-2*asin(sqrt(p2)))
pwr.2p2n.test(h, n1=n1, n2=n2, sig.level=0.05)
```
结果为
```r
              h = 0.04845407
             n1 = 10000
             n2 = 10000
      sig.level = 0.05
          power = 0.9287107
    alternative = two.sided
```
功效$ 1-\beta=0.929 $
如果用10-90的分流比例，样本量相应变为2000和18000
```r
n1 = 2000
n2 = 18000
p1 = 0.10
p2 = 0.115
h = abs(2*asin(sqrt(p1))-2*asin(sqrt(p2)))
pwr.2p2n.test(h, n1=n1, n2=n2, sig.level=0.05)
```
```r
              h = 0.04845407
             n1 = 2000
             n2 = 18000
      sig.level = 0.05
          power = 0.5381772
    alternative = two.sided
```
功效降低为0.538。所以不平衡的分流比例相比均衡的分流效率更低。或者说，必须通过运行时间更长积累更多的样本量才能达到类似的功效。经过尝试，我们发现需要同时将两组提高2.78倍的样本量才能获得等比例分流的统计功效。
```r
n1 = 2000 * 2.78
n2 = 18000 * 2.78
pwr.2p2n.test(h, n1=n1, n2=n2, sig.level=0.05)
```
```r
              h = 0.04845407
             n1 = 5560
             n2 = 50040
      sig.level = 0.05
          power = 0.9288971
    alternative = two.sided
```
我们需要通过两个满足binomial分布的指标的绝对差异的Z检验来理解这个问题。绝对差异的标准误（standard error）如下
$$ SE(p_2-p_1) = \sqrt{\frac{p_1(1-p_1)}{n_1}+\frac{p_2(1-p_2)}{n_2}} $$
当我们收集更多的样本时（$n_1$或$n_2$），SE会减小，进而功效会增加。然而根号里有两项，我们不平衡比例分流的例子实际是：当总样本量一定时，增加$n_1$会导致$n_2$减少，SE的减少比起单纯增加$n_1$时会大大折扣。在给定$p_1, p_2$, 且总样本量$N$一定，即$n_2 = N-n_1$时，令
$$f(n_1) = SE^2=\frac{p_1(1-p_1)}{n_1}+\frac{p_2(1-p_2)}{n_2} = \frac{p_1(1-p_1)}{n_1}+\frac{p_2(1-p_2)}{N-n_1} $$
$f(n_1)$对$n_1$求导，并令$ a = p_1(1-p_1) , b = p_2(1-p_2) $, 有
$$ f^{\prime} (n_1) = \frac{bn_1^2 - a(N-n_1)^2}{(N-n_1)^2n_1^2} $$
我们知道当$ f^{\prime} (n_1) = 0$时，$f(n_1)$最小。在样本量不为0时，恒有$(N-n_1)^2n_1^2 >0$。 由于我们进行的是假设检验，在不能拒绝原假设时$p_2=p_1$, 因此$a=b$, 从而可以由$ f^{\prime} (n_1) = 0$推导出$n_1 = N/2$。即在给定总样本量时，平衡分流对应指标差异的standard error最小，因而统计功效最大。

### 实验中调整流量
上文中提到，在实验进行中调整流量比例有可能引入偏误。我们先通过一个仿真模拟的例子来看:
假设我们在实验开始第一天50-50平均分流为A组和B组，A组有1000个访问用户，转化了250个；B组有1000个访问用户，转化了200个。即A，B组的转化率分别为25%, 20%。从第二天开始，流量调整为A组90%，B组10%。调整后第二天到第七天，A组每天有1800个访问用户，90个转化；而B组有200个访问用户，8个转化，且均无重复访问。第二到七天的转化率分别为5%, 4%。看上去，A似乎更优。但是综合七天的效果, A组有11800个访问（1000+1800*6），790个转化，转化率为6.7%; B组优2200个访问，248个转化，11.2%的转化率。B组胜出。所以即便A组每天转化率都高，最终胜出的确是B组。
这个现象是辛普森悖论。T. Crook, et al.[1]也提到了一个类似的例子。这里问题的本质就是实验的干预变量和随时间变化的访问用户（转化率）分布不再是独立的：对A组来说，转化率更低的第2-7天的样本的权重在1-7天的整个样本中更高，对其影响地更多。

上述例子中我们假设没有重复访问的用户。但在包含复访的现实场景中，更常见的非等比例改变分流的影响却是改变复访用户的抽样构成。一旦访问用户被分配到某个分组的一个桶，他/她就会在实验期间只进入这个分组。所以，假设实验一开始按80%对照组，20%实验组的比例进行分配，一段时间后调整为50-50。那么所有首次进入实验的用户都会按新比例分流。然而在改变之前进入实验的用户分桶后已被固定到某个分组中了，在这里例子里，复访的用户仍旧按80-20的比例进入两组。而复访的用户相比首访的用户在指标上很有可能有不同的表现，对两组的影响就开始不同了。

注意：在实验进行中改变分流比例只涉及到分配比例的改变。整体实验流量的增加并同比例扩流各组, 不会影响实验结果。比如一开始进用10%的整体流量进行实验，A，B两组各分配5%的整体流量。一段时间后打算扩充到整体的50%流量，A，B组仍然按50-50，即25%的整体流量分配，则不会出现上述提到的问题。

### 扩展
对于用不均衡的分流比例或者在实验中调整分流比例的初衷，最常见的说法是：直觉感觉实验组效果会远远好于对照组，或者实验进行了一段时间（但尚未达到预先设计的时长），发现实验组效果不错，为了给整体业务带来更好的作用，加大实验组的比例。

如果希望实验尽早结束，在Frequentist框架下，可以采用一些加快结束的实验的方法，如sequential testing。如果对指标带有先验的信息，或者把实验初期小流量测试的结果建立指标的先验抽样分布（Prior distribution），并希望这种先验的判断在一定程度和数据一起决定最后的决策，则应进行Bayesian A/B testing。比如假设我们考察某个比例类指标$p$并假设其满足Binomial分布；在小流量验证时A组100个访问用户转化了10个，B组99个访问用户转化了11个，在扩流实验时我们把这些数据当成我们的先验知识，并假设
$$ p_a \sim Beta(10, 90), p_b \sim Beta(11, 88) $$
在实验中结合收集的数据的Likelihood, 得到Posterior Distribution。再根据两组指标的Posterior Distribution去得到$Pr(p_a > p_b)。详细信息可参考E. Miller[2]

### 参考文献
[1] Crook, Thomas & Frasca, Brian & Kohavi, Ron & Longbotham, Roger. (2009). Seven pitfalls to avoid when running controlled experiments on the web. 1105-1114. 10.1145/1557019.1557139. 
[2] https://www.evanmiller.org/bayesian-ab-testing.html
